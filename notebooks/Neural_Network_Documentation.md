# üìò ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Neural Network ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô Brent

## ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡πÉ‡∏ä‡πâ Neural Network (Multi-layer Perceptron) ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡∏î‡∏¥‡∏ö Brent ‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏Ç‡∏≠‡∏á‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô Brent ‡πÅ‡∏•‡∏∞ WTI

---

## üìä Workflow Diagram (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)

```mermaid
flowchart TD
    subgraph "1Ô∏è‚É£ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        A[Import Libraries<br/>‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ] --> B[Load Data<br/>‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô]
        B --> C[Data Exploration<br/>‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô]
        C --> D[Check Missing Values<br/>‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ]
    end

    subgraph "2Ô∏è‚É£ Feature Engineering"
        D --> E[Create Lagged Features<br/>‡∏™‡∏£‡πâ‡∏≤‡∏á features ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á]
        E --> F[Create Moving Averages<br/>‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà]
        F --> G[Create Target Variable<br/>‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢]
    end

    subgraph "3Ô∏è‚É£ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Training"
        G --> H[Select Features<br/>‡πÄ‡∏•‡∏∑‡∏≠‡∏Å features ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ]
        H --> I[Train-Val-Test Split<br/>‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 70-15-15]
        I --> J[Normalize Features<br/>Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•]
    end

    subgraph "4Ô∏è‚É£ ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Train Model"
        J --> K[Create MLP Model<br/>‡∏™‡∏£‡πâ‡∏≤‡∏á Neural Network]
        K --> L[Train Model<br/>Training ‡πÇ‡∏°‡πÄ‡∏î‡∏•]
    end

    subgraph "5Ô∏è‚É£ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•"
        L --> M[Make Predictions<br/>‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå]
        M --> N[Calculate Metrics<br/>‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì RMSE, MAE, R¬≤]
        N --> O[Visualize Results<br/>‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå]
    end

    style A fill:#e1f5fe
    style K fill:#fff3e0
    style N fill:#e8f5e9
```

---

## üì¶ ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà 1: Import Libraries (‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ)

```python
# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ pandas ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö DataFrame
import pandas as pd

# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ numpy ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ array
import numpy as np

# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ matplotlib ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏•‡∏∞‡πÅ‡∏ú‡∏ô‡∏†‡∏π‡∏°‡∏¥
import matplotlib.pyplot as plt

# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ seaborn ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡∏Å‡∏ß‡πà‡∏≤ matplotlib
import seaborn as sns

# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ MLPRegressor - Neural Network ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô Regression
# MLP = Multi-layer Perceptron ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô Neural Network ‡πÅ‡∏ö‡∏ö Feedforward
from sklearn.neural_network import MLPRegressor

# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ StandardScaler ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏õ‡∏£‡∏±‡∏ö Mean=0, Std=1)
from sklearn.preprocessing import StandardScaler

# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ metrics ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏•:
# - mean_squared_error: ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
# - mean_absolute_error: ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢  
# - r2_score: ‡∏Ñ‡πà‡∏≤ R¬≤ ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ warnings ‡πÅ‡∏•‡∏∞‡∏õ‡∏¥‡∏î warnings ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
import warnings
warnings.filterwarnings('ignore')

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏£‡∏≤‡∏ü‡πÉ‡∏´‡πâ‡∏°‡∏µ grid ‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß
sns.set_style('whitegrid')

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô 12x6 ‡∏ô‡∏¥‡πâ‡∏ß
plt.rcParams['figure.figsize'] = (12, 6)
```

---

## üì¶ ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà 2: Load Data (‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)

```python
# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV
# ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Brent (suffix _x) ‡πÅ‡∏•‡∏∞ WTI (suffix _y)
df = pd.read_csv('../data/processed/merged_oil_prices.csv')

# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'date' ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏ô‡∏¥‡∏î datetime ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢
df['date'] = pd.to_datetime(df['date'])

# ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏õ‡πÉ‡∏´‡∏°‡πà
# reset_index(drop=True) ‡∏•‡∏ö index ‡πÄ‡∏î‡∏¥‡∏°‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á index ‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 0
df = df.sort_values('date').reset_index(drop=True)

# ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
print(f'‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(df)} ‡πÅ‡∏ñ‡∏ß')

# ‡πÅ‡∏™‡∏î‡∏á‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏ß‡∏±‡∏ô‡πÅ‡∏£‡∏Å ‡∏ñ‡∏∂‡∏á ‡∏ß‡∏±‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)
print(f'‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤: {df["date"].min()} ‡∏ñ‡∏∂‡∏á {df["date"].max()}')

# ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
print(df.columns.tolist())

# ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
df.head()
```

---

## üì¶ ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà 3: Data Statistics (‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)

```python
# ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
# - count: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# - mean: ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
# - std: ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
# - min/max: ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î/‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
# - 25%/50%/75%: ‡∏Ñ‡πà‡∏≤ percentile ‡∏ó‡∏µ‡πà 25, 50 (median), 75
df.describe()
```

---

## üì¶ ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà 4: Check Missing Values (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ)

```python
# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡πà‡∏≤ missing (NaN/null) ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
# isnull() ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ True/False ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ã‡∏•‡∏•‡πå
# sum() ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô True (‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ)
print(df.isnull().sum())

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ missing ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
# sum().sum() ‡∏£‡∏ß‡∏°‡∏Ñ‡πà‡∏≤ missing ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
if df.isnull().sum().sum() == 0:
    print('‚úì ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ missing ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•')
else:
    print('‚úó ‡∏û‡∏ö‡∏Ñ‡πà‡∏≤ missing ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•')
```

---

## üì¶ ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà 5: Feature Engineering (‡∏™‡∏£‡πâ‡∏≤‡∏á Features)

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤ DataFrame ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
data = df.copy()

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Lagged Features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Brent (‡∏£‡∏≤‡∏Ñ‡∏≤‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á)
# shift(lag) ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á lag ‡πÅ‡∏ñ‡∏ß (‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏≠‡∏î‡∏µ‡∏ï)
for lag in [1, 3, 5, 7]:
    # brent_lag_1 = ‡∏£‡∏≤‡∏Ñ‡∏≤ Brent ‡πÄ‡∏°‡∏∑‡πà‡∏≠ 1 ‡∏ß‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô
    # brent_lag_3 = ‡∏£‡∏≤‡∏Ñ‡∏≤ Brent ‡πÄ‡∏°‡∏∑‡πà‡∏≠ 3 ‡∏ß‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô (‡πÅ‡∏•‡∏∞‡∏ï‡πà‡∏≠‡πÑ‡∏õ)
    data[f'brent_lag_{lag}'] = data['close_x'].shift(lag)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Lagged Features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö WTI
for lag in [1, 3, 5, 7]:
    # wti_lag_1 = ‡∏£‡∏≤‡∏Ñ‡∏≤ WTI ‡πÄ‡∏°‡∏∑‡πà‡∏≠ 1 ‡∏ß‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô
    data[f'wti_lag_{lag}'] = data['close_y'].shift(lag)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Moving Averages ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Brent
# rolling(window=5) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á 5 ‡∏ß‡∏±‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á
data['brent_ma_5'] = data['close_x'].rolling(window=5).mean()
data['brent_ma_10'] = data['close_x'].rolling(window=10).mean()

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Moving Averages ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö WTI
data['wti_ma_5'] = data['close_y'].rolling(window=5).mean()
data['wti_ma_10'] = data['close_y'].rolling(window=10).mean()

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Target Variable (‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢)
# shift(-1) ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏∂‡πâ‡∏ô 1 ‡πÅ‡∏ñ‡∏ß = ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ß‡∏±‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
# ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
data['target'] = data['close_x'].shift(-1)

# ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ NaN ‡∏≠‡∏≠‡∏Å (‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ shift ‡πÅ‡∏•‡∏∞ rolling)
# ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡πÜ ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ lag, ‡πÅ‡∏ñ‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ target
data_clean = data.dropna().reset_index(drop=True)
```

---

## üì¶ ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà 6: Select Features (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Features)

```python
# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ Features ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
feature_columns = [
    # Brent Features (‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á)
    'close_x', 'open_x', 'high_x', 'low_x', 'average_x',
    'brent_lag_1', 'brent_lag_3', 'brent_lag_5', 'brent_lag_7',
    'brent_ma_5', 'brent_ma_10',
    
    # WTI Features (‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô WTI ‡∏°‡∏±‡∏Å‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö Brent)
    'close_y', 'open_y', 'high_y', 'low_y', 'average_y',
    'wti_lag_1', 'wti_lag_3', 'wti_lag_5', 'wti_lag_7',
    'wti_ma_5', 'wti_ma_10'
]

# X = Features (‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏≠‡∏¥‡∏™‡∏£‡∏∞) - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
X = data_clean[feature_columns]

# y = Target (‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ï‡∏≤‡∏°) - ‡∏£‡∏≤‡∏Ñ‡∏≤ Brent ‡∏ß‡∏±‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
y = data_clean['target']
```

---

## üì¶ ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà 7: Train-Validation-Test Split (‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)

```python
# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Time Series (‡πÑ‡∏°‡πà‡∏™‡∏∏‡πà‡∏° ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤)
# Training: 70% - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
train_size = int(len(X) * 0.7)

# Validation: 15% - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
val_size = int(len(X) * 0.15)

# Training Set (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 0 ‡∏ñ‡∏∂‡∏á train_size)
X_train = X[:train_size]
y_train = y[:train_size]

# Validation Set (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train_size ‡∏ñ‡∏∂‡∏á train_size+val_size)
X_val = X[train_size:train_size+val_size]
y_val = y[train_size:train_size+val_size]

# Test Set (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 15%)
# ‡πÉ‡∏ä‡πâ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
X_test = X[train_size+val_size:]
y_test = y[train_size+val_size:]
```

> [!IMPORTANT]
> **‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ö‡πà‡∏á‡πÅ‡∏ö‡∏ö Time Series?**  
> ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤ ‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡πà‡∏°‡πÅ‡∏ö‡πà‡∏á‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• "‡πÄ‡∏´‡πá‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï" ‡∏ã‡∏∂‡πà‡∏á‡∏ú‡∏¥‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£ ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

---

## üì¶ ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà 8: Normalize Features (Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á StandardScaler ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# StandardScaler ‡∏à‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏°‡∏µ Mean=0 ‡πÅ‡∏•‡∏∞ Std=1
scaler = StandardScaler()

# fit_transform ‡∏Å‡∏±‡∏ö Training Set
# fit: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì mean ‡πÅ‡∏•‡∏∞ std ‡∏à‡∏≤‡∏Å Training Set
# transform: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ mean ‡πÅ‡∏•‡∏∞ std ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ
X_train_scaled = scaler.fit_transform(X_train)

# transform ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (‡πÑ‡∏°‡πà fit) ‡∏Å‡∏±‡∏ö Validation ‡πÅ‡∏•‡∏∞ Test Set
# ‡πÉ‡∏ä‡πâ mean ‡πÅ‡∏•‡∏∞ std ‡∏à‡∏≤‡∏Å Training Set ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á data leakage
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)
```

> [!TIP]
> **‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á Normalize?**  
> Neural Network ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô ‡∏ñ‡πâ‡∏≤ features ‡∏°‡∏µ‡∏™‡πÄ‡∏Å‡∏•‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å (‡πÄ‡∏ä‡πà‡∏ô ‡∏£‡∏≤‡∏Ñ‡∏≤ 70 ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö volume 10000) ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏ä‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠

---

## üì¶ ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà 9: Create Neural Network Model (‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•)

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á Multi-Layer Perceptron Regressor
model = MLPRegressor(
    # hidden_layer_sizes: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô neurons ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ hidden layer
    # (64, 32, 16) = Layer 1: 64 neurons, Layer 2: 32 neurons, Layer 3: 16 neurons
    hidden_layer_sizes=(64, 32, 16),
    
    # activation: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô activation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hidden layers
    # 'relu' = ReLU (Rectified Linear Unit) - max(0, x)
    activation='relu',
    
    # solver: ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö optimize weights
    # 'adam' = Adaptive Moment Estimation - ‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö Deep Learning
    solver='adam',
    
    # alpha: ‡∏Ñ‡πà‡∏≤ L2 regularization ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô overfitting
    # ‡∏Ñ‡πà‡∏≤‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏•‡∏î overfitting ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à underfit
    alpha=0.001,
    
    # batch_size: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô samples ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ iteration
    # 'auto' = ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ï‡∏≤‡∏°‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°
    batch_size='auto',
    
    # learning_rate: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡∏£‡∏±‡∏ö learning rate
    # 'adaptive' = ‡∏•‡∏î learning rate ‡πÄ‡∏°‡∏∑‡πà‡∏≠ loss ‡πÑ‡∏°‡πà‡∏•‡∏î
    learning_rate='adaptive',
    
    # learning_rate_init: ‡∏Ñ‡πà‡∏≤ learning rate ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    learning_rate_init=0.001,
    
    # max_iter: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô epochs ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    max_iter=200,
    
    # shuffle: ‡∏™‡∏∏‡πà‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ epoch
    shuffle=True,
    
    # random_state: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î seed ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ
    random_state=42,
    
    # early_stopping: ‡∏´‡∏¢‡∏∏‡∏î training ‡πÄ‡∏°‡∏∑‡πà‡∏≠ validation loss ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
    early_stopping=True,
    
    # validation_fraction: ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö validation (‡∏à‡∏≤‡∏Å training set)
    validation_fraction=0.15,
    
    # n_iter_no_change: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô epochs ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏¢‡∏∏‡∏î (‡∏ñ‡πâ‡∏≤ loss ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)
    n_iter_no_change=10,
    
    # verbose: ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• training progress
    verbose=True
)
```

---

## üì¶ ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà 10: Train and Evaluate (Training ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•)

### Training Model

```python
# Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ Training Data
# fit() ‡∏Ñ‡∏∑‡∏≠‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# X_train_scaled: features ‡∏ó‡∏µ‡πà normalized ‡πÅ‡∏•‡πâ‡∏ß
# y_train: ‡∏£‡∏≤‡∏Ñ‡∏≤ Brent ‡∏ß‡∏±‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
model.fit(X_train_scaled, y_train)

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô iterations ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á
print(f'‡∏à‡∏≥‡∏ô‡∏ß‡∏ô iterations ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {model.n_iter_}')

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Loss ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≥‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)
print(f'Loss ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: {model.loss_:.4f}')
```

### Make Predictions (‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå)

```python
# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ dataset
# predict() ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà train ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤ y ‡∏à‡∏≤‡∏Å X

y_train_pred = model.predict(X_train_scaled)  # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ö‡∏ô Training Set
y_val_pred = model.predict(X_val_scaled)      # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ö‡∏ô Validation Set
y_test_pred = model.predict(X_test_scaled)    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ö‡∏ô Test Set
```

### Calculate Metrics (‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î)

```python
def calculate_metrics(y_true, y_pred, dataset_name):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏•
    
    Parameters:
    - y_true: ‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á
    - y_pred: ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
    - dataset_name: ‡∏ä‡∏∑‡πà‡∏≠ dataset
    """
    
    # RMSE (Root Mean Squared Error)
    # ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Å‡∏±‡∏ö error ‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    # ‡∏Ñ‡πà‡∏≤‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≥‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    
    # MAE (Mean Absolute Error)
    # ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
    # ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢: ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏Å‡∏µ‡πà‡∏î‡∏≠‡∏•‡∏•‡∏≤‡∏£‡πå
    mae = mean_absolute_error(y_true, y_pred)
    
    # R¬≤ (Coefficient of Determination)
    # ‡∏ß‡∏±‡∏î‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏Å‡∏µ‡πà %
    # ‡∏Ñ‡πà‡∏≤ 1 = ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö, ‡∏Ñ‡πà‡∏≤ 0 = ‡πÑ‡∏°‡πà‡∏î‡∏µ‡πÄ‡∏•‡∏¢
    r2 = r2_score(y_true, y_pred)
    
    print(f'{dataset_name} Results:')
    print(f'  RMSE: ${rmse:.4f}')  # ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏î‡∏≠‡∏•‡∏•‡∏≤‡∏£‡πå
    print(f'  MAE:  ${mae:.4f}')
    print(f'  R¬≤:   {r2:.4f}')
    
    return {'RMSE': rmse, 'MAE': mae, 'R2': r2}

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å dataset
train_metrics = calculate_metrics(y_train, y_train_pred, 'Training Set')
val_metrics = calculate_metrics(y_val, y_val_pred, 'Validation Set')
test_metrics = calculate_metrics(y_test, y_test_pred, 'Test Set')
```

---

## ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£

### ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?

| ‡∏•‡∏≥‡∏î‡∏±‡∏ö | ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£ | ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ |
|:---:|---------|:-----:|----------|
| 1 | Import Libraries | ‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á | ‡πÉ‡∏ä‡πâ‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML |
| 2 | Load & Explore Data | ‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á | ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô processing |
| 3 | Check Missing Values | ‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á | ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á features |
| 4 | Feature Engineering | ‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á | Lag + MA ‡πÄ‡∏õ‡πá‡∏ô features ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Time Series |
| 5 | Train-Val-Test Split | ‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á | ‡πÅ‡∏ö‡πà‡∏á‡πÅ‡∏ö‡∏ö Time Series ‡πÑ‡∏°‡πà shuffle |
| 6 | Normalization | ‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á | fit ‡∏ö‡∏ô train, transform ‡∏ö‡∏ô val/test |
| 7 | Model Architecture | ‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á | 3 hidden layers ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° |
| 8 | Early Stopping | ‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á | ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô overfitting |
| 9 | Evaluation Metrics | ‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á | RMSE, MAE, R¬≤ ‡πÄ‡∏õ‡πá‡∏ô metrics ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô |

### ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

> [!NOTE]
> **‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Training:**
> - Training R¬≤ = 0.9903 (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• training ‡πÑ‡∏î‡πâ 99%)
> - Test R¬≤ = 0.8545 (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• test ‡πÑ‡∏î‡πâ 85%)
> - MAE ‡∏ö‡∏ô Test = $0.98 (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô ~$1)

> [!TIP]
> **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á:**
> 1. ‡∏≠‡∏≤‡∏à‡∏•‡∏≠‡∏á LSTM/GRU ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Time Series ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ MLP
> 2. ‡πÄ‡∏û‡∏¥‡πà‡∏° features ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÄ‡∏ä‡πà‡∏ô Volume, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô
> 3. ‡∏•‡∏≠‡∏á Cross-Validation ‡πÅ‡∏ö‡∏ö Time Series

---

## üìà ‡∏™‡∏£‡∏∏‡∏õ

‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏ô `Neural_Network.ipynb` ‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà:

1. **‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•** - ‡πÇ‡∏´‡∏•‡∏î, ‡∏™‡∏≥‡∏£‡∏ß‡∏à, ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Missing Values
2. **Feature Engineering** - ‡∏™‡∏£‡πâ‡∏≤‡∏á Lagged Features ‡πÅ‡∏•‡∏∞ Moving Averages
3. **‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•** - ‡πÅ‡∏ö‡πà‡∏á‡πÅ‡∏ö‡∏ö Time Series (70-15-15)
4. **Normalization** - ‡πÉ‡∏ä‡πâ StandardScaler ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
5. **Model Training** - ‡πÉ‡∏ä‡πâ MLP ‡∏û‡∏£‡πâ‡∏≠‡∏° Early Stopping
6. **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•** - ‡πÉ‡∏ä‡πâ RMSE, MAE, R¬≤ ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
